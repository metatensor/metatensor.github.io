{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Custom architectures with ``ModuleMap``\n\n.. py:currentmodule:: metatensor.torch.learn.nn\n\nThis tutorial demonstrates how to build custom architectures compatible with\n``TensorMap`` objects by combining native ``torch.nn`` modules with metatensor-learn's\n``ModuleMap``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Prior to this tutorial, it is recommended to read the tutorial on `using\n    convenience modules <learn-tutorial-nn-modules-basic>`, as this tutorial builds on\n    the concepts introduced there.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n\nimport torch\n\nimport metatensor.torch as mts\nfrom metatensor.torch import Labels, TensorMap\nfrom metatensor.torch.learn.nn import Linear, ModuleMap\n\n\ntorch.manual_seed(42)\ntorch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nThe previous tutorials cover how to use metatensor learn's ``nn`` convenience modules\nto build simple multi-layer perceptrons and their equivariance-preserving analogs. Now\nwe will explore the use of a special module called ``ModuleMap`` that allows users to\nwrap any native torch module in a ``TensorMap`` compatible manner.\n\nThis is useful for building arbitrary architectures containing layers more\ncomplex than those found in the standard available layers: namely ``Linear``,\n``Tanh``, ``ReLU``, ``SiLU`` and ``LayerNorm`` and their equivariant\ncounterparts.\n\nFirst we need to create some dummy data in the :py:class:`TensorMap` format,\nwith multiple :py:class:`TensorBlock` objects. Here we will focus on\nunconstrained architectures, as opposed to equivariance preserving ones. The\nprinciples in the latter case will be similar, as long as care is taken to\nbuild architectures with equivarince-preserving transformations.\n\nLet's start by defining a random tensor that we will treat as some\nintermediate representation. We will build a multi-layer perceptron to\ntransform this tensor into a prediction. Here we will define a 3-block tensor\nmap, with variables with the in and out dimensions for each block.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 100\nin_features = [64, 128, 256]\nout_features = [1, 2, 3]\n\nfeature_tensormap = TensorMap(\n    keys=Labels([\"key\"], torch.arange(len(out_features)).reshape(-1, 1)),\n    blocks=[\n        mts.block_from_array(torch.randn(n_samples, in_feats))\n        for in_feats in in_features\n    ],\n)\n\ntarget_tensormap = TensorMap(\n    keys=Labels([\"key\"], torch.arange(len(out_features)).reshape(-1, 1)),\n    blocks=[\n        mts.block_from_array(torch.randn(n_samples, out_feats))\n        for out_feats in out_features\n    ],\n)\nprint(\"features:\", feature_tensormap)\nprint(\"target:\", target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Starting simple\n\nLet's start with a simple linear layer, but this time constructed manually using\n``ModuleMap``. Here we want a linear layer for each block, with the correct in and out\nfeature shapes. The result will be a module that is equivalent to the\n``metatensor.torch.learn.nn.Linear`` module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "in_keys = feature_tensormap.keys\n\nmodules = []\nfor key in in_keys:\n    module = torch.nn.Linear(\n        in_features=len(feature_tensormap[key].properties),\n        out_features=len(target_tensormap[key].properties),\n        bias=True,\n    )\n    modules.append(module)\n\n# initialize the ModuleMap with the input keys, list of modules, and the output\n# property labels' metadata.\nlinear_mmap = ModuleMap(\n    in_keys,\n    modules,\n    out_properties=[target_tensormap[key].properties for key in in_keys],\n)\nprint(linear_mmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``ModuleMap`` automatically handles the forward pass for each block indexed by\nthe ``in_keys`` used to initialize it. In cases where the input contains more\nkeys/blocks than what is present in the ``in_keys` field, the forward pass\nwill only be applied to the blocks that are present in the input. The output\nwill be a new ``TensorMap`` with the same keys as the input, now with the\ncorrect output metadata.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# apply the ModuleMap to the whole tensor map of features\nprediction_full = linear_mmap(feature_tensormap)\n\n# filter the features to only contain one of the blocks,\n# and pass it through the ModuleMap\nprediction_subset = linear_mmap(\n    mts.filter_blocks(\n        feature_tensormap, Labels([\"key\"], torch.tensor([1]).reshape(-1, 1))\n    )\n)\n\nprint(prediction_full.keys, prediction_full.blocks())\nprint(prediction_subset.keys, prediction_subset.blocks())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define a loss function and run a training loop. This is the same as in\nthe previous tutorials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define a custom loss function for TensorMaps that computes the squared error and\n# reduces by a summation operation\nclass TensorMapLoss(torch.nn.Module):\n    \"\"\"\n    A custom loss function for TensorMaps that computes the squared error and reduces by\n    sum.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, _input: TensorMap, target: TensorMap) -> torch.Tensor:\n        \"\"\"\n        Computes the total squared error between the ``_input`` and ``target``\n        TensorMaps.\n        \"\"\"\n        # inputs and targets should have the same metadata over all axes\n        assert mts.equal_metadata(_input, target)\n\n        squared_loss = 0\n        for key in _input.keys:\n            squared_loss += torch.sum((_input[key].values - target[key].values) ** 2)\n\n        return squared_loss\n\n\n# construct a basic training loop. For brevity we will not use datasets or dataloaders.\ndef training_loop(\n    model: torch.nn.Module,\n    loss_fn: torch.nn.Module,\n    features: Union[torch.Tensor, TensorMap],\n    targets: Union[torch.Tensor, TensorMap],\n) -> None:\n    \"\"\"A basic training loop for a model and loss function.\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    for epoch in range(501):\n        optimizer.zero_grad()\n\n        predictions = model(features)\n\n        if isinstance(predictions, torch.ScriptObject):\n            # assume a TensorMap and check metadata is equivalent\n            assert mts.equal_metadata(predictions, targets)\n\n        loss = loss_fn(predictions, targets)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"epoch: {epoch}, loss: {loss}\")\n\n\nloss_fn_mts = TensorMapLoss()\n\nprint(\"with NN = [Linear]\")\ntraining_loop(linear_mmap, loss_fn_mts, feature_tensormap, target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More complex architectures\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defining more complicated architectures is a matter of building\n# ``torch.nn.Sequential`` objects for each block, and wrapping them into a single\n# ModuleMap.\n\nhidden_layer_width = 32\n\nmodules = []\nfor key in in_keys:\n    module = torch.nn.Sequential(\n        torch.nn.LayerNorm(len(feature_tensormap[key].properties)),\n        torch.nn.Linear(\n            in_features=len(feature_tensormap[key].properties),\n            out_features=hidden_layer_width,\n            bias=True,\n        ),\n        torch.nn.ReLU(),\n        torch.nn.Linear(\n            in_features=hidden_layer_width,\n            out_features=len(target_tensormap[key].properties),\n            bias=True,\n        ),\n        torch.nn.Tanh(),\n    )\n    modules.append(module)\n\n# initialize the ModuleMap as in the previous section.\ncustom_mmap = ModuleMap(\n    in_keys,\n    modules,\n    out_properties=[target_tensormap[key].properties for key in in_keys],\n)\nprint(custom_mmap)\n\nprint(\"with NN = [LayerNorm, Linear, ReLU, Linear, Tanh]\")\ntraining_loop(custom_mmap, loss_fn_mts, feature_tensormap, target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ModuleMap objects can also be wrapped in a ``torch.nn.torch.nn.Module`` to\nallow construction of complex architectures. For instance, we can have a\n\"ResNet\"-style neural network module that takes a ModuleMap and applies it,\nthen sums with some residual connections. Wikipedia has a good summary and\ndiagram of this architectural motif, see:\nhttps://en.wikipedia.org/wiki/Residual_neural_network .\n\nTo do the latter step, we can combine application of the ``ModuleMap`` with a\n``Linear`` convenience layer from metatensor-learn, and the sparse addition operation\nfrom ``metatensor-operations`` to build a complex architecture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ResidualNetwork(torch.nn.Module):\n    def __init__(\n        self,\n        in_keys: Labels,\n        in_features: List[int],\n        out_properties: List[Labels],\n    ) -> None:\n        super().__init__()\n\n        # Build the module map as before\n        hidden_layer_width = 32\n        modules = []\n        for in_feats, out_props in zip(in_features, out_properties):\n            module = torch.nn.Sequential(\n                torch.nn.LayerNorm(in_feats),\n                torch.nn.Linear(\n                    in_features=in_feats,\n                    out_features=hidden_layer_width,\n                    bias=True,\n                ),\n                torch.nn.ReLU(),\n                torch.nn.Linear(\n                    in_features=hidden_layer_width,\n                    out_features=len(out_props),\n                    bias=True,\n                ),\n                torch.nn.Tanh(),\n            )\n            modules.append(module)\n\n        self.module_map = ModuleMap(\n            in_keys,\n            modules,\n            out_properties=out_properties,\n        )\n\n        # build the input projection layer\n        self.projection = Linear(\n            in_keys=in_keys,\n            in_features=in_features,\n            out_properties=out_properties,\n            bias=True,\n        )\n\n    def forward(self, features: TensorMap) -> TensorMap:\n        # apply the module map to the features\n        prediction = self.module_map(features)\n\n        # apply the projection layer to the features\n        residual = self.projection(features)\n\n        # add the prediction and residual together using the sparse addition\n        # from metatensor-operations\n        return mts.add(prediction, residual)\n\n\nmodel = ResidualNetwork(\n    in_keys=in_keys,\n    in_features=in_features,\n    out_properties=[block.properties for block in target_tensormap],\n)\nprint(\"with NN = [LayerNorm, Linear, ReLU, Linear, Tanh] plus residual connections\")\ntraining_loop(model, loss_fn_mts, feature_tensormap, target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial we have seen how to build custom architectures using ``ModuleMap``.\nThis allows for arbitrary architectures to be built, as long as the metadata is\npreserved. We have also seen how to build a custom module that wraps a ``ModuleMap``\nand adds residual connections.\n\nThe key takeaway is that ``ModuleMap`` can be used to wrap any combination of native\n``torch.nn`` modules to make them compatible with ``TensorMap``. In combination with\nconvenience layers seen in the tutorial `nn modules basic\n<learn-tutorial-nn-modules-basic>`, and sparse-data operations from\n``metatensor-operations``, complex architectures can be built with ease.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}