{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Convenience ``nn`` modules\n\n.. py:currentmodule:: metatensor.torch.learn.nn\n\nThis example demonstrates the use of convenience modules in metatensor-learn to build\nsimple multi-layer perceptrons.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The convenience modules introduced in this tutorial are designed to be used to\n    prototype new architectures for simple models. If you already have more complex\n    models, you can also wrap them in ``ModuleMap`` objects to make them compatible with\n    metatensor. This is covered in a later tutorial `using module maps\n    <learn-tutorial-nn-using-modulemap>`.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Union\n\nimport torch\n\nimport metatensor.torch as mts\nfrom metatensor.torch import Labels, TensorMap\nfrom metatensor.torch.learn import nn\n\n\ntorch.manual_seed(42)\ntorch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to native ``torch.nn`` modules\n\nmetatensor-learn's neural network modules are designed to be\n:py:class:`TensorMap`-compatible analogues to the torch API. Before looking into the\n``metatensor-learn version``, it is instructive to recap torch's native ``nn`` modules\nand recall how they work.\n\nFirst, let's define a random tensor that we will treat as an intermediate\nrepresentation. We will build a multi-layer perceptron to transform this tensor into a\nprediction.\n\nLet's say we have 100 samples, the size of the input latent space is 128, and the\ntarget property is of dimension 1. We will start with a simple linear layer to map the\nlatent representation to a prediction of the target property:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 100\nin_features = 128\nout_features = 1\nfeature_tensor = torch.randn(n_samples, in_features)\n\n# define a dummy target\ntarget_tensor = torch.randn(n_samples, 1)\n\n# initialize the torch linear layer\nlinear_torch = torch.nn.Linear(in_features, out_features, bias=True)\n\n# define a loss function\nloss_fn_torch = torch.nn.MSELoss(reduction=\"sum\")\n\n\n# construct a basic training loop. For brevity we will not use datasets or dataloaders.\ndef training_loop(\n    model,\n    loss_fn,\n    features: Union[torch.Tensor, TensorMap],\n    targets: Union[torch.Tensor, TensorMap],\n) -> None:\n    \"\"\"A basic training loop for a model and it's loss function.\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    for epoch in range(1001):\n        optimizer.zero_grad()\n\n        predictions = model(features)\n\n        if isinstance(predictions, torch.ScriptObject):\n            # assume a TensorMap and check that the\n            # metadata is equivalent\n            assert mts.equal_metadata(predictions, targets)\n\n        loss = loss_fn(predictions, targets)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"epoch: {epoch}, loss: {loss}\")\n\n\nprint(\"with NN = [Linear]\")\ntraining_loop(linear_torch, loss_fn_torch, feature_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now run the training loop, this time with a nonlinear multi-layer perceptron using\n``torch.nn.Sequential``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hidden_layer_width = 64\nmlp_torch = torch.nn.Sequential(\n    torch.nn.Linear(in_features, hidden_layer_width),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_layer_width, out_features),\n)\n\n# train again\nprint(\"with NN = [Linear, ReLU, Linear]\")\ntraining_loop(mlp_torch, loss_fn_torch, feature_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the metatensor-learn ``nn`` layers\n\nNow we're ready to see how the ``nn`` module in ``metatensor-learn`` works.\n\nFirst we create some dummy data, this time in :py:class:`TensorMap` format.\nStarting simple, we will define a :py:class:`TensorMap` with only one\n:py:class:`TensorBlock`, containing the latent space features from above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_tensormap = TensorMap(\n    keys=Labels.single(),\n    blocks=[mts.block_from_array(feature_tensor)],\n)\n\ntarget_tensormap = TensorMap(\n    keys=Labels.single(),\n    blocks=[mts.block_from_array(target_tensor)],\n)\n\n# for supervised learning, inputs and labels must have the same metadata for all axes\n# except the properties dimension, as this is the dimension that is transformed by the\n# neural network.\nif mts.equal_metadata(\n    feature_tensormap, target_tensormap, check=[\"samples\", \"components\"]\n):\n    print(\"metadata check passed!\")\nelse:\n    raise ValueError(\n        \"input and output TensorMaps must have matching keys, samples, \"\n        \"and components metadata\"\n    )\n\n# use metatensor-learn's Linear layer. We need to pass the target property's labels so\n# that the TensorMap for predictions is annotated with the correct metadata.\nin_keys = feature_tensormap.keys\nlinear_mts = nn.Linear(\n    in_keys=in_keys,\n    in_features=in_features,\n    out_properties=[block.properties for block in target_tensormap],\n    bias=True,\n)\n\n\n# define a custom loss function over TensorMaps that computes the squared error and\n# reduces by summation\nclass TensorMapLoss(nn.Module):\n    \"\"\"\n    A custom loss function for TensorMaps that computes the squared error and\n    reduces by summation.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, input: TensorMap, target: TensorMap) -> torch.Tensor:\n        \"\"\"\n        Computes the total squared error between the ``input`` and ``target``\n        TensorMaps.\n        \"\"\"\n        # input and target should have the same metadata across all axes\n        assert mts.equal_metadata(input, target)\n\n        squared_loss = 0\n        for key in input.keys:\n            squared_loss += torch.sum((input[key].values - target[key].values) ** 2)\n\n        return squared_loss\n\n\nloss_fn_mts = TensorMapLoss()\n\n# run the training loop\nprint(\"with NN = [Linear]\")\ntraining_loop(linear_mts, loss_fn_mts, feature_tensormap, target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now construct a nonlinear MLP instead. Here we use metatensor-learn's Sequential\nmodule, along with some nonlinear activation modules. We only need to pass the\nproperties metadata for the output layer, for the hidden layers, we just pass the\nlayer dimension\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp_mts = nn.Sequential(\n    in_keys,\n    nn.Linear(\n        in_keys=in_keys,\n        in_features=in_features,\n        out_features=hidden_layer_width,\n        bias=True,\n    ),\n    nn.ReLU(in_keys=in_keys),  # can also use Tanh or SiLU\n    nn.Linear(\n        in_keys=in_keys,\n        in_features=hidden_layer_width,\n        out_properties=[block.properties for block in target_tensormap],\n        bias=True,\n    ),\n)\n\n\n# run the training loop\nprint(\"with NN = [Linear, ReLU, Linear]\")\ntraining_loop(mlp_mts, loss_fn_mts, feature_tensormap, target_tensormap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThis tutorial introduced the convenience modules in metatensor-learn for building\nsimple neural networks. As we've seen, the API is similar to native ``torch.nn`` and\nthe ``TensorMap`` data type can be easily switched in place for torch Tensors in\nexisting training loops with minimal changes.\n\nCombined with other learning utilities to construct Datasets and Dataloaders, covered\nin `basic <learn-tutorial-dataset-dataloader>` and `advanced\n<learn-tutorial-indexed-dataset-dataloader>` tutorials, metatensor-learn provides a\npowerful framework for building and training machine learning models based on the\nTensorMap data format.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}