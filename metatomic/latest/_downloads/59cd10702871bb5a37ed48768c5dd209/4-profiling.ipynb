{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Profiling your models\n\n.. py:currentmodule:: metatomic.torch\n\nDo you feel like your model is too slow? Do you want to make it faster? Instead of\nguessing which part of the code is responsible for the slowdown, you should profile your\ncode to learn how much time is spent in each function and where to focus any\noptimization efforts.\n\nIn this tutorial you'll learn how to profile your model using the PyTorch\nprofiler, how to read the output of the profiler, and how to add your own labels\nfor new functions/steps in your model's forward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional\n\nimport ase.build\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom metatensor.torch import Labels, TensorBlock, TensorMap\n\nfrom metatomic.torch import (\n    AtomisticModel,\n    ModelCapabilities,\n    ModelMetadata,\n    ModelOutput,\n    System,\n)\nfrom metatomic.torch.ase_calculator import MetatomicCalculator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When profiling your code, it is important to run the model on a representative system\nto ensure you are actually exercising the behavior of your model at the right scale.\nHere we'll use a relatively large system with many atoms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primitive = ase.build.bulk(name=\"C\", crystalstructure=\"diamond\", a=3.567)\natoms = ase.build.make_supercell(primitive, 10 * np.eye(3))\nprint(f\"We have {len(atoms)} atoms in our system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same ``HarmonicModel`` as in the `previous tutorial\n<atomistic-tutorial-md>` as our machine learning potential.\n\n.. raw:: html\n\n    <details>\n    <summary>Click to see the definition of HarmonicModel</summary>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class HarmonicModel(torch.nn.Module):\n    def __init__(self, force_constant: float, equilibrium_positions: torch.Tensor):\n        \"\"\"Create an ``HarmonicModel``.\n\n        :param force_constant: force constant, in ``energy unit / (length unit)^2``\n        :param equilibrium_positions: torch tensor with shape ``n x 3``, containing the\n            equilibrium positions of all atoms\n        \"\"\"\n        super().__init__()\n        assert force_constant > 0\n        self.force_constant = force_constant\n        self.equilibrium_positions = equilibrium_positions\n\n    def forward(\n        self,\n        systems: List[System],\n        outputs: Dict[str, ModelOutput],\n        selected_atoms: Optional[Labels],\n    ) -> Dict[str, TensorMap]:\n        if list(outputs.keys()) != [\"energy\"]:\n            raise ValueError(\n                \"this model can only compute 'energy', but `outputs` contains other \"\n                f\"keys: {', '.join(outputs.keys())}\"\n            )\n\n        # we don't want to worry about selected_atoms yet\n        if selected_atoms is not None:\n            raise NotImplementedError(\"selected_atoms is not implemented\")\n\n        if outputs[\"energy\"].per_atom:\n            raise NotImplementedError(\"per atom energy is not implemented\")\n\n        # compute the energy for each system by adding together the energy for each atom\n        energy = torch.zeros((len(systems), 1), dtype=systems[0].positions.dtype)\n        for i, system in enumerate(systems):\n            assert len(system) == self.equilibrium_positions.shape[0]\n            r0 = self.equilibrium_positions\n            energy[i] += torch.sum(self.force_constant * (system.positions - r0) ** 2)\n\n        # add metadata to the output\n        block = TensorBlock(\n            values=energy,\n            samples=Labels(\"system\", torch.arange(len(systems)).reshape(-1, 1)),\n            components=[],\n            properties=Labels(\"energy\", torch.tensor([[0]])),\n        )\n        return {\n            \"energy\": TensorMap(keys=Labels(\"_\", torch.tensor([[0]])), blocks=[block])\n        }\n\n\nmodel = HarmonicModel(\n    force_constant=3.14159265358979323846,\n    equilibrium_positions=torch.tensor(atoms.positions),\n)\n\ncapabilities = ModelCapabilities(\n    outputs={\n        \"energy\": ModelOutput(quantity=\"energy\", unit=\"eV\", per_atom=False),\n    },\n    atomic_types=[6],\n    interaction_range=0.0,\n    length_unit=\"Angstrom\",\n    supported_devices=[\"cpu\"],\n    dtype=\"float32\",\n)\n\nmetadata = ModelMetadata()\nwrapper = AtomisticModel(model.eval(), metadata, capabilities)\n\nwrapper.export(\"exported-model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n    </details>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are trying to profile your own model, you can start here and create\n``MetatomicCalculator`` with your own model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling energy calculation\n\nWe will start with an energy-only calculator, which can be enabled with\n``do_gradients_with_energy=False``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "atoms.calc = MetatomicCalculator(\"exported-model.pt\", do_gradients_with_energy=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before trying to profile the code, it is a good idea to run it a couple of times to\nallow torch to warmup internally.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n    # force the model to re-run everytime, otherwise ASE caches calculation results\n    atoms.rattle(1e-6)\n    atoms.get_potential_energy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can run code using :py:func:`torch.profiler.profile` to collect statistic on\nhow long each function takes to run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "atoms.rattle(1e-6)\nwith torch.profiler.profile() as energy_profiler:\n    atoms.get_potential_energy()\n\nprint(energy_profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are a couple of interesting things to see here. First the total runtime of the\ncode is shown in the bottom; and then the most costly functions are visible on top,\none line per function. For each function, ``Self CPU`` refers to the time spent in\nthis function **excluding** any called functions; and ``CPU total`` refers to the time\nspent in this function, **including** called functions.\n\nFor more options to record operations and display outputs, please refer to the\n[official documentation for PyTorch profiler](https://pytorch.org/docs/stable/profiler.html).\n\nHere, ``Model::forward`` indicates the time taken by your model's ``forward()``.\nAnything starting with ``aten::`` comes from operations on torch tensors, typically\nwith the same function name as the corresponding torch functions (e.g.\n``aten::arange`` is :py:func:`torch.arange`). We can also see some internal functions\nfrom metatomic, with the name staring with ``AtomisticModel::`` for\n:py:class:`AtomisticModel`; and ``MetatomicCalculator::`` for\n:py:class:`ase_calculator.MetatomicCalculator`.\n\nIf you want to see more details on the internal steps taken by your model, you\ncan add :py:func:`torch.profiler.record_function`\n(https://pytorch.org/docs/stable/generated/torch.autograd.profiler.record_function.html)\ninside your model code to give names to different steps in the calculation.\nThis is how we internally set names such as ``Model::forward`` or\n``MetatomicCalculator::prepare_inputs`` above.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling forces calculation\n\nLet us now do the same, but while also computing the forces for this system.\nThis mean we should now see some time spent in the ``backward()`` function, on\ntop of everything else.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "atoms.calc = MetatomicCalculator(\"exported-model.pt\")\n\n# warmup\nfor _ in range(10):\n    atoms.rattle(1e-6)\n    atoms.get_forces()\n\natoms.rattle(1e-6)\nwith torch.profiler.profile() as forces_profiler:\n    atoms.get_forces()\n\nprint(forces_profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize this data in another way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = forces_profiler.key_averages()\nevents = sorted(events, key=lambda u: u.self_cpu_time_total, reverse=True)\ntotal_cpu_time = sum(map(lambda u: u.self_cpu_time_total, events))\n\nbottom = 0.0\nfor event in events:\n    self_time = event.self_cpu_time_total\n    name = event.key\n    if len(name) > 30:\n        name = name[:12] + \"[...]\" + name[-12:]\n\n    if self_time > 0.03 * total_cpu_time:\n        plt.bar(0, self_time, bottom=bottom, label=name)\n        bottom += self_time\n    else:\n        plt.bar(0, total_cpu_time - bottom, bottom=bottom, label=\"others\")\n        break\n\nplt.legend()\nplt.xticks([])\nplt.xlim(0, 1)\nplt.ylabel(\"self time / \u00b5s\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}