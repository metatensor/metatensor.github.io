{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Datasets and data loaders\n\n.. py:currentmodule:: metatensor.learn.data\n\nThis tutorial shows how to define :py:class:`Dataset` and :py:class:`DataLoader`\ncompatible with PyTorch and containing metatensor data (i.e. data stored in\n:py:class:`metatensor.torch.TensorMap`) in addition to more usual types of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport torch\n\nfrom metatensor.learn.data import DataLoader, Dataset\nfrom metatensor.torch import Labels, TensorBlock, TensorMap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a simple dummy dataset with two fields, named 'x' and 'y'. Every field in\nthe `Dataset` must be a list of objects corresponding to the different samples in this\ndataset.\n\nLet's define our x data as a list of random tensors, and our y data as a list of\nintegers enumerating the samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 5\nx_data = [torch.randn(3) for _ in range(n_samples)]\ny_data = [i for i in range(n_samples)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-memory dataset\n\nWe are ready to build out first dataset. The simplest use case is when all data is in\nmemory. In this case, we can pass the data directly to the :py:class:`Dataset`\nconstructor as keyword arguments, named and ordered according to how we want the data\nto be returned when we access samples in the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "in_memory_dataset = Dataset(x=x_data, y=y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now access samples in the dataset. The returned object is a named tuple with\nfields corresponding to the keyword arguments given to the :py:class:``Dataset`\nconstructor (here ``x`` and ``y``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(in_memory_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can also iterate over the samples in the dataset as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for sample in in_memory_dataset:\n    print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Any number of named data fields can be passed to the Dataset constructor, as long as\nthey are all uniquely named, and are all lists of the same length. The elements of\neach list can be any type of object (integer, string, torch Tensor, etc.), as long as\nit is the type same for all samples in the respective field.\n\nFor example, here we are creating a dataset of torch tensors (``x``), integers\n(``y``), and strings (``z``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bigger_dataset = Dataset(x=x_data, y=y_data, z=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nprint(bigger_dataset[0])\nprint(\"Sample 4, z field:\", bigger_dataset[4].z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mixed in-memory / on-disk dataset\n\nNow suppose we have a large dataset, where the x data is too large to fit in\nmemory. In this case, we might want to lazily load data when training a model\nwith minibatches.\n\nLet's save the x data to disk to simulate this use case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a directory to save the dummy x data to disk\nos.makedirs(\"data\", exist_ok=True)\n\nfor i, x in enumerate(x_data):\n    torch.save(x, f\"data/x_{i}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order for the x data to be loaded lazily, we need to give the ``Dataset`` a\n``load`` function that loads a single sample into memory. This can a function of\narbitrary complexity, taking a single argument which is the numeric index (between\n``0`` and ``len(dataset)``) of the sample to load\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_x(sample_id):\n    \"\"\"\n    Loads the x data for the sample indexed by `sample_id` from disk and returns the\n    object in memory\n    \"\"\"\n    print(f\"loading x for sample {sample_id}\")\n    return torch.load(f\"data/x_{sample_id}.pt\")\n\n\nprint(\"load_x called with sample index 0:\", load_x(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now when we define a dataset, the 'x' data field can be passed as a callable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mixed_dataset = Dataset(x=load_x, y=y_data)\nprint(mixed_dataset[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## On-disk dataset\n\nFinally, suppose we have a large dataset, where both the x and y data are too large to\nfit in memory. In this case, we might want to lazily load all data when training a\nmodel with minibatches.\n\nLet's save the y data to disk as well to simulate this use case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, y in enumerate(y_data):\n    torch.save(y, f\"data/y_{i}.pt\")\n\n\ndef load_y(sample_id):\n    \"\"\"\n    Loads the y data for the sample indexed by `sample_id` from disk and\n    returns the object in memory\n    \"\"\"\n    print(f\"loading y for sample {sample_id}\")\n    return torch.load(f\"data/y_{sample_id}.pt\")\n\n\nprint(\"load_y called with sample index 0:\", load_y(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now when we define a dataset, as all the fields are to be lazily loaded, we need to\nindicate how many samples are in the dataset with the ``size`` argument.\n\nInternally, the Dataset class infers the unique sample indexes as a continuous integer\nsequence starting from 0 to ``size - 1`` (inclusive). In this case, sample indexes are\ntherefore [0, 1, 2, 3, 4]. These indexes are used to lazily load the data upon access.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "on_disk_dataset = Dataset(x=load_x, y=load_y, size=n_samples)\nprint(on_disk_dataset[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Dataloader\n\nNow let's see how we can use the Dataset class to build a DataLoader.\n\nMetatensor's ``DataLoader`` class is a wrapper around the PyTorch ``DataLoader``\nclass, and as such can be initialized with a ``Dataset`` object. It will also inherit\nall of the default arguments from the PyTorch DataLoader class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "in_memory_dataloader = DataLoader(in_memory_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now iterate over the DataLoader to access batches of samples from the\ndataset. With no arguments passed, the default batch size is 1 and the samples\nare not shuffled.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for batch in in_memory_dataloader:\n    print(batch.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an alternative syntax, the data fields can be unpacked into separate variables in\nthe for loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for x, y in in_memory_dataloader:\n    print(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also pass arguments to the DataLoader constructor to change the batch\nsize and shuffling of the samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "in_memory_dataloader = DataLoader(in_memory_dataset, batch_size=2, shuffle=True)\n\nfor batch in in_memory_dataloader:\n    print(batch.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loaders for cross-validation\n\nOne can use the usual torch :py:func:`torch.utils.data.random_split` function to split\na ``Dataset`` into train, validation, and test subsets for cross-validation purposes.\n``DataLoader`` s can then be constructed for each subset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Perform a random train/val/test split of the Dataset,\n# in the relative proportions (60% / 20% / 20%)\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    in_memory_dataset, [0.6, 0.2, 0.2]\n)\n\n# Construct DataLoaders for each subset\ntrain_dataloader = DataLoader(train_dataset)\nval_dataloader = DataLoader(val_dataset)\ntest_dataloader = DataLoader(test_dataset)\n\n# As the Dataset was initialized with 5 samples, the split should be 3:1:1\nprint(f\"Dataset size: {len(on_disk_dataset)}\")\nprint(f\"Training set size: {len(train_dataloader)}\")\nprint(f\"Validation set size: {len(val_dataloader)}\")\nprint(f\"Test set size: {len(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with :py:class:`torch.Tensor` and :py:class:`metatensor.torch.TensorMap`\n\nAs the :py:class:`Dataset` and :py:class:`DataLoader` classes exist to interface\nmetatensor and torch, let's explore how they behave when using\n:py:class:`torch.Tensor` and :py:class:`metatensor.torch.TensorMap` objects as the\ndata.\n\nWe'll consider some dummy data consisting of the following fields:\n\n- **descriptor**: a list of random TensorMap objects\n- **scalar**: a list of random floats\n- **vector**: a list of random torch Tensors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a dummy descriptor as a TensorMap\ndescriptor = [\n    TensorMap(\n        keys=Labels(\n            names=[\"key_1\", \"key_2\"],\n            values=torch.tensor([[1, 2]]),\n        ),\n        blocks=[\n            TensorBlock(\n                values=torch.randn((1, 3)),\n                samples=Labels(\"sample_id\", torch.tensor([[sample_id]])),\n                components=[],\n                properties=Labels(\"p\", torch.tensor([[1], [4], [5]])),\n            )\n        ],\n    )\n    for sample_id in range(n_samples)\n]\n\n# Create dummy scalar and vectorial target properties as torch Tensors\nscalar = [float(torch.rand(1, 1)) for _ in range(n_samples)]\nvector = [torch.rand(1, 3) for _ in range(n_samples)]\n\n# Build the Dataset\ndataset = Dataset(\n    scalar=scalar,\n    vector=vector,\n    descriptor=descriptor,\n)\nprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging samples in a batch\n\nAs is typically customary when working with torch Tensors, we want to vertically stack\nthe samples in a minibatch into a single Tensor object. This allows passing a single\nTensor object to a model, rather than a tuple of Tensor objects. In a similar way,\nsparse data stored in metatensor TensorMap objects can also be vertically stacked,\ni.e. joined along the samples axis, into a single TensorMap object.\n\nThe default ``collate_fn`` used by :py:class:`DataLoader`\n(:py:func:`metatensor.learn.data.group_and_join`), vstacks (respectively joins along\nthe samples axis) data fields that correspond :py:class:`torch.Tensor` (respectively\n:py:class:`metatensor.torch.TensorMap`). For all other data types, the data is left as\ntuple containing all samples in the current batch in order.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 2\ndataloader = DataLoader(dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can look at a single ``Batch`` object (i.e. a named tuple, returned by the\n``DataLoader.__iter__()``) to see this in action.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n\n# TensorMaps for each sample in the batch joined along the samples axis\n# into a single TensorMap\nprint(\"batch.descriptor =\", batch.descriptor)\n\n# `scalar` data are float objects, so are just grouped and returned in a tuple\nprint(\"batch.scalar =\", batch.scalar)\nassert len(batch.scalar) == batch_size\n\n# `vector` data are torch Tensors, so are vertically stacked into a single\n# Tensor\nprint(\"batch.vector =\", batch.vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced functionality: IndexedDataset\n\nWhat if we wanted to explicitly define the sample indexes used to store and access\nsamples in the dataset? See the next tutorial,\n`learn-tutorial-indexed-dataset-dataloader`, for more details!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}